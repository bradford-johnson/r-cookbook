[
  {
    "objectID": "cleaning-data.html",
    "href": "cleaning-data.html",
    "title": "",
    "section": "",
    "text": "Cleaning data is one of the most important steps to any data analytics project. Cleaning data can involve anything from changing the case of characters from uppercase to lowercase to removing outliers from a data set, or even figuring out what to do with missing values. Having clean data is essential for making recommendations to stakeholders as your analysis can only be as strong as your data is clean. So very clean and structured data may lead you to entirely different insights than if you where to not clean it at all.\nThere are countless ways to clean your data in R, and I will show you different ways I have cleaned up data sets.\n\n\nHowever, first I think it is important to have clean and concise but descriptive headers, when dealing with tabular data. There have been many times where I load some data into R, and the headers are all uppercase, contain spaces, or something else that makes them annoying to work with. The janitor package is great because it can help clean up the headers (or column names) so they are easier to work with. I will load in the readr package to import a hand crafted .csv that I made for this, the dplyr package so I can pipe the data into functions.\n\n# load packages\nlibrary(janitor)\nlibrary(readr)\nlibrary(dplyr)\n\n# read in csv with no changes\ndirty_df <- read_csv('janitor-example.csv')\n\n# read in csv but with janitor and dplyr functions\nclean_df <- read_csv('janitor-example.csv') %>%\n  clean_names() %>%\n  mutate(weather_condition = w_ea_th_er_conditions) %>%\n  mutate(avg_temp_f = temp_f) %>%\n  mutate(weekday = day_of_the_week) %>%\n  select(weekday, avg_temp_f, weather_condition)\n\nHere is how both the original data frame and cleaned look.\n\n# head()\ndirty_df %>% \n  head()\n\n# A tibble: 6 x 3\n  `DAY OF THE WEEK` `TEMP F` `WEaThEr CONDITIONS`\n  <chr>                <dbl> <chr>               \n1 Monday                  98 sunny               \n2 Tuesday                 95 sunny               \n3 Wednesday               70 cloudy              \n4 Thursday                85 sunny               \n5 Friday                  83 sunny               \n6 Saturday                85 sunny               \n\nclean_df %>%\n  head()\n\n# A tibble: 6 x 3\n  weekday   avg_temp_f weather_condition\n  <chr>          <dbl> <chr>            \n1 Monday            98 sunny            \n2 Tuesday           95 sunny            \n3 Wednesday         70 cloudy           \n4 Thursday          85 sunny            \n5 Friday            83 sunny            \n6 Saturday          85 sunny"
  },
  {
    "objectID": "collecting-data.html",
    "href": "collecting-data.html",
    "title": "",
    "section": "",
    "text": "The first step to any data analytics project is collecting data. This step may not be always necessary as in many cases the data has already been collected. For example in my current role we get daily analytics emails each morning that contain the data from the previous day. So if this data is relevant and all we need for our analysis we can skip the “collecting data” stage.\nBut what if we wanted to gain different insights that require new data? Then we would need to collect new data. Depending on what I was doing I could add more data to the current data I have, or create a new set of data all together.\n\n\nThe way we collect data varies from using a pencil and notepad, to an automated process that saves every entry in the cloud. One way I have used R to collect data is by web scraping. Using the rvest package can allow you to collect data from the internet with minimal effort (avoid the constant copy/pasting).\nBelow I will show a simple script using the rvest, lubridate and tidyverse packages that can scrape us some data from Steam’s game stats page. Steam is a video game distribution service, and we will scrape a couple columns from their live Top games by current player count table.\n\n# load packages\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(lubridate)\n\n# link to get data from\nlink = \"https://store.steampowered.com/stats/\" \n\n# read webpage at the above link\npage = read_html(link) \n\n# scrape top 100 games by current players\ngame = page %>% html_nodes(\".gameLink\") %>% html_text()  \n\n# scrape number of players for each game \ncurrent_players = page %>% html_nodes(\"td:nth-child(1) .currentServers\") %>% html_text() \n\n# put both game and player data into a data frame\ndf = data.frame(game, current_players) \n\n# get current date\ncurrent_date <- as_datetime(Sys.Date())\n\n# update data frame with mutated column that adds current_date\ndf <- df %>% \n  mutate(date = current_date)\n\nNow lets see the first 6 rows of our new data frame.\n\nhead(df)\n\n                              game current_players       date\n1 Counter-Strike: Global Offensive         501,662 2022-08-27\n2                           Dota 2         380,653 2022-08-27\n3                     Apex Legends         217,662 2022-08-27\n4                         Lost Ark         217,374 2022-08-27\n5                        Destiny 2         157,950 2022-08-27\n6              PUBG: BATTLEGROUNDS         132,755 2022-08-27"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "This is a cookbook for R created by Bradford Johnson.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\nThis Quarto book contains vignettes of R."
  }
]