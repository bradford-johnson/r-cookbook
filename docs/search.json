[
  {
    "objectID": "cleaning-data.html",
    "href": "cleaning-data.html",
    "title": "",
    "section": "",
    "text": "Cleaning data is one of the most important steps to any data analytics project. Cleaning data can involve anything from changing the case of characters from uppercase to lowercase to removing outliers from a data set, or even figuring out what to do with missing values. Having clean data is essential for making recommendations to stakeholders as your analysis can only be as strong as your data is clean. So very clean and structured data may lead you to entirely different insights than if you where to not clean it at all.\nThere are countless ways to clean your data in R, and I will show you different ways I have cleaned up data sets.\n\n\nHowever, first I think it is important to have clean and concise but descriptive headers, when dealing with tabular data. There have been many times where I load some data into R, and the headers are all uppercase, contain spaces, or something else that makes them annoying to work with. The janitor package is great because it can help clean up the headers (or column names) so they are easier to work with. I will load in the readr package to import a hand crafted .csv that I made as an example. I will load in the dplyr package so I can pipe the data into functions.\n\n# load packages\nlibrary(janitor)\nlibrary(readr)\nlibrary(dplyr)\n\n# read in csv with no changes\ndirty_df <- read_csv('janitor-example.csv')\n\n# read in csv but with janitor and dplyr functions\nclean_df <- read_csv('janitor-example.csv') %>%\n  clean_names() %>%\n  mutate(weather_condition = w_ea_th_er_conditions) %>%\n  mutate(avg_temp_f = temp_f) %>%\n  mutate(weekday = day_of_the_week) %>%\n  select(weekday, avg_temp_f, weather_condition)\n\nHere is how the original data frame and cleaned data frame look. The column names are now easier to work with, and better understood.\n\n# head()\ndirty_df %>% \n  head()\n\n# A tibble: 6 x 3\n  `DAY OF THE WEEK` `TEMP F` `WEaThEr CONDITIONS`\n  <chr>                <dbl> <chr>               \n1 Monday                  98 sunny               \n2 Tuesday                 95 sunny               \n3 Wednesday               70 cloudy              \n4 Thursday                85 sunny               \n5 Friday                  83 sunny               \n6 Saturday                85 sunny               \n\nclean_df %>%\n  head()\n\n# A tibble: 6 x 3\n  weekday   avg_temp_f weather_condition\n  <chr>          <dbl> <chr>            \n1 Monday            98 sunny            \n2 Tuesday           95 sunny            \n3 Wednesday         70 cloudy           \n4 Thursday          85 sunny            \n5 Friday            83 sunny            \n6 Saturday          85 sunny            \n\n\n\n\n\nMany times you may need to filter data, for example if you only want to see observations on a specific weekday, or with certain values. That is easy to do and with the dplyr package you will be able to really be creative with filtering, creating additional columns, and much more.\nFor some of these examples I will use some data sets that come with R, the first data set we will look at is chickwts which looks at baby chick weights and feed types. I am going to summarize the counts for the feeds to quickly see all the options. Then I will filter some of the feeds as they are no longer available for my stakeholder in this scenario.\n\n# load packages and data \nlibrary(dplyr)\n\nchick_df <- chickwts\n\n# counts for each feed type\nchick_df %>%\n  group_by(feed) %>%\n  summarise(n = n())\n\n# A tibble: 6 x 2\n  feed          n\n  <fct>     <int>\n1 casein       12\n2 horsebean    10\n3 linseed      12\n4 meatmeal     11\n5 soybean      14\n6 sunflower    12\n\n\n\n# keep feeds: sunflower, soybean, linseed\nchick_feeds <- chick_df %>%\n  filter(feed == 'sunflower' | feed == 'soybean' | feed == 'linseed')\n\n# counts for each type of feed\nchick_feeds %>%\n  group_by(feed) %>%\n  summarise(n = n())\n\n# A tibble: 3 x 2\n  feed          n\n  <fct>     <int>\n1 linseed      12\n2 soybean      14\n3 sunflower    12\n\n\nHow about a different filter that returns all rows that have weights below 200 units, and are linseed or horsebean feeds.\n\nchick_df %>%\n  filter(weight < 200 & feed == 'linseed' | weight < 200 & feed == 'horsebean')\n\n   weight      feed\n1     179 horsebean\n2     160 horsebean\n3     136 horsebean\n4     168 horsebean\n5     108 horsebean\n6     124 horsebean\n7     143 horsebean\n8     140 horsebean\n9     181   linseed\n10    141   linseed\n11    148   linseed\n12    169   linseed\n\n\nNow I am going to use the mutate() function to create a new column, and this column will be used to classify a chicks weight category based on some predetermined values.\n\nFor this example lets say these are the weight classes:\n\nweight < 200 - underweight\nweight >= 200 & weight <= 300 - normal\nweight > 300 - overweight\n\n\n\n# mutate() and casewhen()\nchick_classes <- chick_df %>%\n  mutate(weight_class = case_when(\n    weight < 200 ~ 'underweight',\n    weight >= 200 & weight <= 300 ~ 'normal',\n    weight > 300 ~ 'overweight'\n    ))\n\n# view results\nchick_classes\n\n   weight      feed weight_class\n1     179 horsebean  underweight\n2     160 horsebean  underweight\n3     136 horsebean  underweight\n4     227 horsebean       normal\n5     217 horsebean       normal\n6     168 horsebean  underweight\n7     108 horsebean  underweight\n8     124 horsebean  underweight\n9     143 horsebean  underweight\n10    140 horsebean  underweight\n11    309   linseed   overweight\n12    229   linseed       normal\n13    181   linseed  underweight\n14    141   linseed  underweight\n15    260   linseed       normal\n16    203   linseed       normal\n17    148   linseed  underweight\n18    169   linseed  underweight\n19    213   linseed       normal\n20    257   linseed       normal\n21    244   linseed       normal\n22    271   linseed       normal\n23    243   soybean       normal\n24    230   soybean       normal\n25    248   soybean       normal\n26    327   soybean   overweight\n27    329   soybean   overweight\n28    250   soybean       normal\n29    193   soybean  underweight\n30    271   soybean       normal\n31    316   soybean   overweight\n32    267   soybean       normal\n33    199   soybean  underweight\n34    171   soybean  underweight\n35    158   soybean  underweight\n36    248   soybean       normal\n37    423 sunflower   overweight\n38    340 sunflower   overweight\n39    392 sunflower   overweight\n40    339 sunflower   overweight\n41    341 sunflower   overweight\n42    226 sunflower       normal\n43    320 sunflower   overweight\n44    295 sunflower       normal\n45    334 sunflower   overweight\n46    322 sunflower   overweight\n47    297 sunflower       normal\n48    318 sunflower   overweight\n49    325  meatmeal   overweight\n50    257  meatmeal       normal\n51    303  meatmeal   overweight\n52    315  meatmeal   overweight\n53    380  meatmeal   overweight\n54    153  meatmeal  underweight\n55    263  meatmeal       normal\n56    242  meatmeal       normal\n57    206  meatmeal       normal\n58    344  meatmeal   overweight\n59    258  meatmeal       normal\n60    368    casein   overweight\n61    390    casein   overweight\n62    379    casein   overweight\n63    260    casein       normal\n64    404    casein   overweight\n65    318    casein   overweight\n66    352    casein   overweight\n67    359    casein   overweight\n68    216    casein       normal\n69    222    casein       normal\n70    283    casein       normal\n71    332    casein   overweight\n\nchick_classes %>% \n  group_by(weight_class) %>%\n  summarise(n = n())\n\n# A tibble: 3 x 2\n  weight_class     n\n  <chr>        <int>\n1 normal          28\n2 overweight      26\n3 underweight     17"
  },
  {
    "objectID": "collecting-data.html",
    "href": "collecting-data.html",
    "title": "",
    "section": "",
    "text": "The first step to any data analytics project is collecting data. This step may not be always necessary as in many cases the data has already been collected. For example in my current role we get daily analytics emails each morning that contain the data from the previous day. So if this data is relevant and all we need for our analysis we can skip the “collecting data” stage.\nBut what if we wanted to gain different insights that require new data? Then we would need to collect new data. Depending on what I was doing I could add more data to the current data I have, or create a new set of data all together.\n\n\nThe way we collect data varies from using a pencil and notepad, to an automated process that saves every entry in the cloud. One way I have used R to collect data is by web scraping. Using the rvest package can allow you to collect data from the internet with minimal effort (avoid the constant copy/pasting).\nBelow I will show a simple script using the rvest, lubridate and tidyverse packages that can scrape us some data from Steam’s game stats page. Steam is a video game distribution service, and we will scrape a couple columns from their live Top games by current player count table.\n\n# load packages\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(lubridate)\n\n# link to get data from\nlink = \"https://store.steampowered.com/stats/\" \n\n# read webpage at the above link\npage = read_html(link) \n\n# scrape top 100 games by current players\ngame = page %>% html_nodes(\".gameLink\") %>% html_text()  \n\n# scrape number of players for each game \ncurrent_players = page %>% html_nodes(\"td:nth-child(1) .currentServers\") %>% html_text() \n\n# put both game and player data into a data frame\ndf = data.frame(game, current_players) \n\n# get current date\ncurrent_date <- as_datetime(Sys.Date())\n\n# update data frame with mutated column that adds current_date\ndf <- df %>% \n  mutate(date = current_date)\n\nNow lets see the first 6 rows of our new data frame.\n\nhead(df)\n\n                              game current_players       date\n1 Counter-Strike: Global Offensive         501,662 2022-08-27\n2                           Dota 2         380,653 2022-08-27\n3                     Apex Legends         217,662 2022-08-27\n4                         Lost Ark         217,374 2022-08-27\n5                        Destiny 2         157,950 2022-08-27\n6              PUBG: BATTLEGROUNDS         132,755 2022-08-27"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "This is a cookbook for R created by Bradford Johnson.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\nThis Quarto book contains vignettes of R."
  }
]