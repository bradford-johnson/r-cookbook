[
  {
    "objectID": "visualizing-data.html",
    "href": "visualizing-data.html",
    "title": "3  Visualizing Data",
    "section": "",
    "text": "There are many ways you can visualize data, and selecting a way to visualize your data depends on what kind of data you have. For example, if you have geographic data, then using a map can be an option. The visual you pick also should be effective at telling the story for your stakeholders.\nFor the different visualizations, I will group them based on what they show:"
  },
  {
    "objectID": "visualizing-data.html#distribution",
    "href": "visualizing-data.html#distribution",
    "title": "3  Visualizing Data",
    "section": "3.1 Distribution",
    "text": "3.1 Distribution"
  },
  {
    "objectID": "collecting-data.html",
    "href": "collecting-data.html",
    "title": "1  Collecting Data",
    "section": "",
    "text": "The first step to any data analytics project is collecting data. This step may not be always necessary as in many cases the data has already been collected. For example in my current role we get daily analytics emails each morning that contain the data from the previous day. So if this data is relevant and all we need for our analysis we can skip the “collecting data” stage.\nBut what if we wanted to gain different insights that require new data? Then we would need to collect new data. Depending on what I was doing I could add more data to the current data I have, or create a new set of data all together."
  },
  {
    "objectID": "collecting-data.html#web-scraping",
    "href": "collecting-data.html#web-scraping",
    "title": "1  Collecting Data",
    "section": "1.1 Web Scraping",
    "text": "1.1 Web Scraping\nThe way we collect data varies from using a pencil and notepad, to an automated process that saves every entry in the cloud. One way I have used R to collect data is by web scraping. Using the rvest package can allow you to collect data from the internet with minimal effort (avoid the constant copy/pasting).\nBelow I will show a simple script using the rvest, lubridate and tidyverse packages that can scrape us some data from Steam’s game stats page. Steam is a video game distribution service, and we will scrape a couple columns from their live Top games by current player count table.\n\n# load packages\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(lubridate)\n\n# link to get data from\nlink = \"https://store.steampowered.com/stats/\" \n\n# read webpage at the above link\npage = read_html(link) \n\n# scrape top 100 games by current players\ngame = page %>% html_nodes(\".gameLink\") %>% html_text()  \n\n# scrape number of players for each game \ncurrent_players = page %>% html_nodes(\"td:nth-child(1) .currentServers\") %>% html_text() \n\n# put both game and player data into a data frame\ndf = data.frame(game, current_players) \n\n# get current date\ncurrent_date <- as_datetime(Sys.Date())\n\n# update data frame with mutated column that adds current_date\ndf <- df %>% \n  mutate(date = current_date)\n\nNow lets see the first 6 rows of our new data frame.\n\nhead(df)\n\n                              game current_players       date\n1 Counter-Strike: Global Offensive         479,208 2022-09-04\n2                           Dota 2         453,373 2022-09-04\n3                         Lost Ark         152,280 2022-09-04\n4                     Apex Legends         126,172 2022-09-04\n5                        Destiny 2         124,690 2022-09-04\n6                  Team Fortress 2         110,364 2022-09-04"
  }
]