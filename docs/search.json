[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R-Cookbook",
    "section": "",
    "text": "This is a cookbook for R created by Bradford Johnson. You can use the menu on the left the navigate the different chapters and the Table of contents on the right to see different sections for the current page you are on.\nThis Quarto book contains vignettes of R code, and this book is not meant to be a full step by step process for everything you need to know in order to analyze data in R.\n\n\nGitHub Profile Link\nProject Repository Link\n\n\n\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "collecting-data.html",
    "href": "collecting-data.html",
    "title": "1  Collecting Data",
    "section": "",
    "text": "The first step to any data analytics project is collecting data. This step may not be always necessary as in many cases the data has already been collected. For example in my current role we get daily analytics emails each morning that contain the data from the previous day. So if this data is relevant and all we need for our analysis we can skip the “collecting data” stage.\nBut what if we wanted to gain different insights that require new data? Then we would need to collect new data. Depending on what I was doing I could add more data to the current data I have, or create a new set of data all together."
  },
  {
    "objectID": "collecting-data.html#web-scraping",
    "href": "collecting-data.html#web-scraping",
    "title": "1  Collecting Data",
    "section": "1.1 Web Scraping",
    "text": "1.1 Web Scraping\nThe way we collect data varies from using a pencil and notepad, to an automated process that saves every entry in the cloud. One way I have used R to collect data is by web scraping. Using the rvest package can allow you to collect data from the internet with minimal effort (avoid the constant copy/pasting).\nBelow I will show a simple script using the rvest, lubridate and tidyverse packages that can scrape us some data from Steam’s game stats page. Steam is a video game distribution service, and we will scrape a couple columns from their live Top games by current player count table.\n\n# load packages\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(lubridate)\n\n# link to get data from\nlink = \"https://store.steampowered.com/stats/\" \n\n# read webpage at the above link\npage = read_html(link) \n\n# scrape top 100 games by current players\ngame = page %>% html_nodes(\".gameLink\") %>% html_text()  \n\n# scrape number of players for each game \ncurrent_players = page %>% html_nodes(\"td:nth-child(1) .currentServers\") %>% html_text() \n\n# put both game and player data into a data frame\ndf = data.frame(game, current_players) \n\n# get current date\ncurrent_date <- as_datetime(Sys.Date())\n\n# update data frame with mutated column that adds current_date\ndf <- df %>% \n  mutate(date = current_date)\n\nNow lets see the first 6 rows of our new data frame.\n\nhead(df)\n\n                              game current_players       date\n1 Counter-Strike: Global Offensive         439,496 2022-09-01\n2                           Dota 2         372,634 2022-09-01\n3                        Destiny 2         111,597 2022-09-01\n4                         Lost Ark         109,504 2022-09-01\n5                     Apex Legends         109,228 2022-09-01\n6                             Rust         102,430 2022-09-01"
  },
  {
    "objectID": "cleaning-data.html",
    "href": "cleaning-data.html",
    "title": "2  Cleaning Data",
    "section": "",
    "text": "Cleaning data is one of the most important steps to any data analytics project. Cleaning data can involve anything from changing the case of characters from uppercase to lowercase to removing outliers from a data set, or even figuring out what to do with missing values. Having clean data is essential for making recommendations to stakeholders, as your analysis can only be as strong as your data is clean. So very clean and structured data may lead you to entirely different insights than if you where to not clean it at all.\nThere are countless ways to clean your data in R, and I will show you different ways I have cleaned up data sets."
  },
  {
    "objectID": "cleaning-data.html#column-names-headers",
    "href": "cleaning-data.html#column-names-headers",
    "title": "2  Cleaning Data",
    "section": "2.1 Column Names (Headers)",
    "text": "2.1 Column Names (Headers)\nHowever, first I think it is important to have clean and concise but descriptive headers, when dealing with tabular data. There have been many times where I load some data into R, and the headers are all uppercase, contain spaces, or something else that makes them annoying to work with. The janitor package is great because it can help clean up the headers (or column names) so they are easier to work with. I will load in the readr package to import a hand crafted .csv that I made as an example. I will load in the dplyr package so I can pipe the data into functions.\n\n# load packages\nlibrary(janitor)\nlibrary(readr)\nlibrary(dplyr)\n\n# read in csv with no changes\ndirty_df <- read_csv('janitor-example.csv')\n\n# read in csv but with janitor and dplyr functions\nclean_df <- read_csv('janitor-example.csv') %>%\n  clean_names() %>%\n  mutate(weather_condition = w_ea_th_er_conditions) %>%\n  mutate(avg_temp_f = temp_f) %>%\n  mutate(weekday = day_of_the_week) %>%\n  select(weekday, avg_temp_f, weather_condition)\n\nHere is how the original data frame and cleaned data frame look. The column names are now easier to work with, and better understood.\n\n# head()\ndirty_df %>% \n  head()\n\n# A tibble: 6 x 3\n  `DAY OF THE WEEK` `TEMP F` `WEaThEr CONDITIONS`\n  <chr>                <dbl> <chr>               \n1 Monday                  98 sunny               \n2 Tuesday                 95 sunny               \n3 Wednesday               70 cloudy              \n4 Thursday                85 sunny               \n5 Friday                  83 sunny               \n6 Saturday                85 sunny               \n\nclean_df %>%\n  head()\n\n# A tibble: 6 x 3\n  weekday   avg_temp_f weather_condition\n  <chr>          <dbl> <chr>            \n1 Monday            98 sunny            \n2 Tuesday           95 sunny            \n3 Wednesday         70 cloudy           \n4 Thursday          85 sunny            \n5 Friday            83 sunny            \n6 Saturday          85 sunny"
  },
  {
    "objectID": "cleaning-data.html#filter-and-mutate-data",
    "href": "cleaning-data.html#filter-and-mutate-data",
    "title": "2  Cleaning Data",
    "section": "2.2 Filter and Mutate Data",
    "text": "2.2 Filter and Mutate Data\nMany times you may need to filter data, for example if you only want to see observations on a specific weekday, or with certain values. That is easy to do and with the dplyr package you will be able to really be creative with filtering, creating additional columns, and much more.\nFor some of these examples I will use some data sets that come with R, the first data set we will look at is chickwts which looks at baby chick weights and feed types. I am going to summarize the counts for the feeds to quickly see all the options. Then I will filter some of the feeds as they are no longer available for my stakeholder in this scenario.\n\n# load packages and data \nlibrary(dplyr)\n\nchick_df <- chickwts\n\n# counts for each feed type\nchick_df %>%\n  group_by(feed) %>%\n  summarise(n = n())\n\n# A tibble: 6 x 2\n  feed          n\n  <fct>     <int>\n1 casein       12\n2 horsebean    10\n3 linseed      12\n4 meatmeal     11\n5 soybean      14\n6 sunflower    12\n\n\n\n# keep feeds: sunflower, soybean, linseed\nchick_feeds <- chick_df %>%\n  filter(feed == 'sunflower' | feed == 'soybean' | feed == 'linseed')\n\n# counts for each type of feed\nchick_feeds %>%\n  group_by(feed) %>%\n  summarise(n = n())\n\n# A tibble: 3 x 2\n  feed          n\n  <fct>     <int>\n1 linseed      12\n2 soybean      14\n3 sunflower    12\n\n\nHow about a different filter that returns all rows that have weights below 200 units, and are linseed or horsebean feeds.\n\nchick_df %>%\n  filter(weight < 200 & feed == 'linseed' | weight < 200 & feed == 'horsebean')\n\n   weight      feed\n1     179 horsebean\n2     160 horsebean\n3     136 horsebean\n4     168 horsebean\n5     108 horsebean\n6     124 horsebean\n7     143 horsebean\n8     140 horsebean\n9     181   linseed\n10    141   linseed\n11    148   linseed\n12    169   linseed\n\n\nNow I am going to use the mutate() function to create a new column, and this column will be used to classify a chicks weight category based on some predetermined values.\n\nFor this example lets say these are the weight classes:\n\nweight < 200 - underweight\nweight >= 200 & weight <= 300 - normal\nweight > 300 - overweight\n\n\n\n# mutate() and casewhen()\nchick_classes <- chick_df %>%\n  mutate(weight_class = case_when(\n    weight < 200 ~ 'underweight',\n    weight >= 200 & weight <= 300 ~ 'normal',\n    weight > 300 ~ 'overweight'\n    ))\n\n# view top 6 and bottom 6 entries\nchick_classes %>% head()\n\n  weight      feed weight_class\n1    179 horsebean  underweight\n2    160 horsebean  underweight\n3    136 horsebean  underweight\n4    227 horsebean       normal\n5    217 horsebean       normal\n6    168 horsebean  underweight\n\nchick_classes %>% tail()\n\n   weight   feed weight_class\n66    352 casein   overweight\n67    359 casein   overweight\n68    216 casein       normal\n69    222 casein       normal\n70    283 casein       normal\n71    332 casein   overweight\n\n# weight class count table \nchick_classes %>% \n  group_by(weight_class) %>%\n  summarise(n = n())\n\n# A tibble: 3 x 2\n  weight_class     n\n  <chr>        <int>\n1 normal          28\n2 overweight      26\n3 underweight     17"
  },
  {
    "objectID": "cleaning-data.html#removing-outliers",
    "href": "cleaning-data.html#removing-outliers",
    "title": "2  Cleaning Data",
    "section": "2.3 Removing Outliers",
    "text": "2.3 Removing Outliers\nThere are different types of methods that you can use to identify and remove outliers. The method I will show is\n\n# removing outliers"
  }
]