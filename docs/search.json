[
  {
    "objectID": "collecting-data.html",
    "href": "collecting-data.html",
    "title": "1  Collecting Data",
    "section": "",
    "text": "The first step to any data analytics project is collecting data. This step may not be always necessary as in many cases the data has already been collected. For example in my current role we get daily analytics emails each morning that contain the data from the previous day. So if this data is relevant and all we need for our analysis we can skip the “collecting data” stage.\nBut what if we wanted to gain different insights that require new data? Then we would need to collect new data. Depending on what I was doing I could add more data to the current data I have, or create a new set of data all together."
  },
  {
    "objectID": "collecting-data.html#connecting-to-postgresql-database",
    "href": "collecting-data.html#connecting-to-postgresql-database",
    "title": "1  Collecting Data",
    "section": "1.1 Connecting to postgreSQL Database",
    "text": "1.1 Connecting to postgreSQL Database\nConnecting to a database can be an effective way to import data into R for analysis. For this example I will be showing how to connect to a postgreSQL database, as that is what I use.\nYou will need the DBI, RPostgres, and dplyr packages.\n\ninstall.packages(\"DBI\")\ninstall.packages(\"RPostgres\")\ninstall.packages(\"dplyr\")\n\nOnce you install the packages you will need to load them.\n\nlibrary(DBI)\nlibrary(RPostgres)\nlibrary(dplyr)\n\nAfter loading the packages you can then connect to your database with this code and your database’s credentials.\n\n# establish connection with postgres data base\ncon <- dbConnect(RPostgres::Postgres(),dbname = '',  \n      # enter name of database in the single quotes\n                 \n      host = 'address', \n# replace 'address' and enter the address of the database inside the single quotes\n                \n      port = 5432, \n# enter the port for the database \n\n      user = 'username', \n# replace 'username' with your username inside single quotes\n                 \n      password = 'password') \n# replace 'password' with your password inside single quotes\n\nAfter establishing your connection, you can then create your SQL query.\n\n# code to make sql query --inside of the double quotes you can create your query\nres <- dbSendQuery(con, \"\n                   SELECT *\n                   FROM \n                   WHERE ;\")\n\nTo execute your query you will then want to write and run this code, using dbFetch() with the name of your query object being the argument, in this case it is res. The results from this query will be saved as the name you assign it, for this code it is called df.\n\n# execute query\ndf <- dbFetch(res) \n\n# this will save the results of the query as an R data frame called 'DF'\n\nOnce you have run the query you can then clean the query and disconnect from the database using these functions and arguments respectively.\n\n# clear query\ndbClearResult(res)\n\n# disconnect from database\ndbDisconnect(con)"
  },
  {
    "objectID": "collecting-data.html#web-scraping",
    "href": "collecting-data.html#web-scraping",
    "title": "1  Collecting Data",
    "section": "1.2 Web Scraping",
    "text": "1.2 Web Scraping\nThe way we collect data varies from using a pencil and notepad, to an automated process that saves every entry in the cloud. One way I have used R to collect data is by web scraping. Using the rvest package can allow you to collect data from the internet with minimal effort (avoid the constant copy/pasting).\nBelow I will show a simple script using the rvest, lubridate and tidyverse packages that can scrape us some data from Steam’s game stats page. Steam is a video game distribution service, and we will scrape a couple columns from their live Top games by current player count table.\nFirst load the packages\n\n# load packages\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(lubridate)\n\nNext you will set up the parameters so rvest knows where to get the data from. The html_nodes can be found using the browser extension SelectorGadget found here. Using this extension you can highlight what you want to web scrape and copy/paste the nodes from SelectorGadget.\n\n# link to get data from\nlink = \"https://store.steampowered.com/stats/\" \n\n# read webpage at the above link\npage = read_html(link) \n\n# scrape top 100 games by current players\ngame = page %>% \n  html_nodes(\".gameLink\") %>% \n  html_text()  \n\n# scrape number of players for each game \ncurrent_players = page %>% \n  html_nodes(\"td:nth-child(1) .currentServers\") %>% \n  html_text() \n\nAfter getting all the data you will want to put it into a data frame to work with it, so you will then use the data.frame() function and add in the data you pulled from online. Below you will see how I am creating the data frame and adding in the current data as a column so I know when I collected this data.\n\n# put both game and player data into a data frame\ndf = data.frame(game, current_players) \n\n# get current date\ncurrent_date <- as_datetime(Sys.Date())\n\n# update data frame with mutated column that adds current_date\ndf <- df %>% \n  mutate(date = current_date)\n\nNow lets see the first 6 rows of our new data frame that we crafted using rvest.\n\nhead(df)\n\n                              game current_players       date\n1 Counter-Strike: Global Offensive         782,859 2022-09-10\n2                           Dota 2         711,840 2022-09-10\n3                     Apex Legends         220,642 2022-09-10\n4              PUBG: BATTLEGROUNDS         213,567 2022-09-10\n5                         Lost Ark         197,303 2022-09-10\n6               Grand Theft Auto V         129,355 2022-09-10"
  },
  {
    "objectID": "cleaning-data.html",
    "href": "cleaning-data.html",
    "title": "2  Cleaning Data",
    "section": "",
    "text": "Cleaning data is one of the most important steps to any data analytics project. Cleaning data can involve anything from changing the case of characters from uppercase to lowercase to removing outliers from a data set, or even figuring out what to do with missing values. Having clean data is essential for making recommendations to stakeholders, as your analysis can only be as strong as your data is clean.\nThere are countless ways to clean your data in R, and I will show you different ways I have cleaned up data sets."
  },
  {
    "objectID": "cleaning-data.html#column-names-headers",
    "href": "cleaning-data.html#column-names-headers",
    "title": "2  Cleaning Data",
    "section": "2.1 Column Names (Headers)",
    "text": "2.1 Column Names (Headers)\nDealing with dirty data is part of being a data analyst, and the janitor package is great because it can help clean up the headers (or column names) so they are easier to work with. I will load in the readr package to import a hand crafted .csv that I made as an example. I will also load in the dplyr package so I can pipe the data into functions.\nHere is how you install and load the packages.\n\ninstall.packages(\"janitor\")\ninstall.packages(\"readr\")\ninstall.packages(\"dplyr\")\n\n\nlibrary(janitor)\nlibrary(readr)\nlibrary(dplyr)\n\nNow I will load in the data frame with dirty column names / headers.\n\n# read in csv with no changes\ndirty_df <- read_csv('janitor-example.csv')\n\nHere is what the dirty data frame looks like.\n\ndirty_df %>% \n  head()\n\n# A tibble: 6 x 3\n  `DAY OF THE WEEK` `TEMP F` `WEaThEr CONDITIONS`\n  <chr>                <dbl> <chr>               \n1 Monday                  98 sunny               \n2 Tuesday                 95 sunny               \n3 Wednesday               70 cloudy              \n4 Thursday                85 sunny               \n5 Friday                  83 sunny               \n6 Saturday                85 sunny               \n\n\nNow using the clean_names() function from the janitor package along with some mutate() functions I will load in the same data frame.\n\n# read in csv but with janitor and dplyr functions\nclean_df <- read_csv('janitor-example.csv') %>%\n  clean_names() %>%\n  mutate(weather_condition = w_ea_th_er_conditions) %>%\n  mutate(avg_temp_f = temp_f) %>%\n  mutate(weekday = day_of_the_week) %>%\n  select(weekday, avg_temp_f, weather_condition)\n\nHere is how the cleaned data frame looks. The column names are now easier to work with, and much better understood.\n\nclean_df %>%\n  head()\n\n# A tibble: 6 x 3\n  weekday   avg_temp_f weather_condition\n  <chr>          <dbl> <chr>            \n1 Monday            98 sunny            \n2 Tuesday           95 sunny            \n3 Wednesday         70 cloudy           \n4 Thursday          85 sunny            \n5 Friday            83 sunny            \n6 Saturday          85 sunny"
  },
  {
    "objectID": "cleaning-data.html#filter-and-mutate-data",
    "href": "cleaning-data.html#filter-and-mutate-data",
    "title": "2  Cleaning Data",
    "section": "2.2 Filter and Mutate Data",
    "text": "2.2 Filter and Mutate Data\nMany times you may need to filter data, for example if you only want to see observations on a specific weekday, or with certain values. That is easy to do and with the dplyr package you will be able to really be creative with filtering, creating additional columns, and much more.\nFor some of these examples I will use some data sets that come with R, the first data set we will look at is chickwts which looks at baby chick weights and feed types. I am going to summarize the counts for the feeds to quickly see all the options. Then I will filter some of the feeds as they are no longer available for my stakeholder in this scenario.\n\n# load packages and data \nlibrary(dplyr)\n\nchick_df <- chickwts\n\n# counts for each feed type\nchick_df %>%\n  group_by(feed) %>%\n  summarise(n = n())\n\n# A tibble: 6 x 2\n  feed          n\n  <fct>     <int>\n1 casein       12\n2 horsebean    10\n3 linseed      12\n4 meatmeal     11\n5 soybean      14\n6 sunflower    12\n\n\n\n# keep feeds: sunflower, soybean, linseed\nchick_feeds <- chick_df %>%\n  filter(feed == 'sunflower' | feed == 'soybean' | feed == 'linseed')\n\n# counts for each type of feed\nchick_feeds %>%\n  group_by(feed) %>%\n  summarise(n = n())\n\n# A tibble: 3 x 2\n  feed          n\n  <fct>     <int>\n1 linseed      12\n2 soybean      14\n3 sunflower    12\n\n\nHow about a different filter that returns all rows that have weights below 200 units, and are linseed or horsebean feeds.\n\nchick_df %>%\n  filter(weight < 200 & feed == 'linseed' | weight < 200 & feed == 'horsebean')\n\n   weight      feed\n1     179 horsebean\n2     160 horsebean\n3     136 horsebean\n4     168 horsebean\n5     108 horsebean\n6     124 horsebean\n7     143 horsebean\n8     140 horsebean\n9     181   linseed\n10    141   linseed\n11    148   linseed\n12    169   linseed\n\n\nNow I am going to use the mutate() function to create a new column, and this column will be used to classify a chicks weight category based on some predetermined values.\n\nFor this example lets say these are the weight classes:\n\nweight < 200 - underweight\nweight >= 200 & weight <= 300 - normal\nweight > 300 - overweight\n\n\n\n# mutate() and casewhen()\nchick_classes <- chick_df %>%\n  mutate(weight_class = case_when(\n    weight < 200 ~ 'underweight',\n    weight >= 200 & weight <= 300 ~ 'normal',\n    weight > 300 ~ 'overweight'\n    ))\n\n# weight class count table \nchick_classes %>% \n  group_by(weight_class) %>%\n  summarise(n = n())\n\n# A tibble: 3 x 2\n  weight_class     n\n  <chr>        <int>\n1 normal          28\n2 overweight      26\n3 underweight     17"
  },
  {
    "objectID": "cleaning-data.html#removing-outliers",
    "href": "cleaning-data.html#removing-outliers",
    "title": "2  Cleaning Data",
    "section": "2.3 Removing Outliers",
    "text": "2.3 Removing Outliers\nThis image is from DataCamp’s learning platform, and it shows a visual of a boxplot, and the outliers on either side of the distribution. To find the outliers on the lower range you do the following equation Q1 - 1.5 x IQR. To find outliers on the upper range you use this equation Q3 + 1.5 * IQR.\n\nYou can use those equations to then filter out the outliers and then continue with your analysis. I have created some data with outliers to show how you would do this in R.\n\n# remove outliers steps\n\n# 1. get Q1 and Q3 \nQ = quantile(outlier_df$var1.1, probs = c(.25,.75), na.rm = FALSE)\n\n# 2. get IQR\niqr = IQR(outlier_df$var1.1)\n\n# 3. get upper and lower ranges\nup <-  Q[2]+1.5*iqr # Upper Range  \nlow <- Q[1]-1.5*iqr # Lower Range\n\n# 4. remove outliers (outlier_df is the name of my data frame, var1.1 is the name of the column that I am removing outliers from)\n\nno_outliers_p <- outlier_df %>%\n  filter(var1.1 > low & var1.1 < up) %>%\n  ggplot(aes(x = var1.1)) +\n  geom_boxplot() + \n  theme_classic() +\n  labs(title = \"Without Outliers\") +\n  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank())\n\n# patchwork to show plots\noutlier_p / no_outliers_p"
  },
  {
    "objectID": "analyzing-data.html",
    "href": "analyzing-data.html",
    "title": "3  Analyzing Data",
    "section": "",
    "text": "Once your data is cleaned up then it is ready to investigate the data in order to find your insights and how significant they are."
  },
  {
    "objectID": "analyzing-data.html#statistical-tests",
    "href": "analyzing-data.html#statistical-tests",
    "title": "3  Analyzing Data",
    "section": "3.1 Statistical Tests",
    "text": "3.1 Statistical Tests\n\n3.1.1 T-Test\nA t-test is a good way to see if there is a difference between two groups and “quantifying” the difference of the groups. Learn more about the t-test.\nLets say I wanted to know if there was a difference between the US and Netherlands when it comes to life expectancy. My next steps would be to create a null hypothesis and the alternative hypothesis. For this type of question the null would be that the averages of both groups are equal, the alternative is that they are not equal.\n\nNull hypothesis: average life expectancy of US = average life expectanvy of the Netherlands\nAlternative hypothesis: average life expectancy of US ≠ average life expectanvy of the Netherlands\n\nNext I want to select my critical value which gives “boundaries” to the acceptance of the test. A common critical value that is used is 0.05, and this means that there is a 5% chance if done 100 times, that I will reject the null hypothesis and accept the alternative. Critical values such as 0.01 is even more accurate as there is a 1% chance of rejecting the null, but a 99% chance of retaining the null, so choosing a critical value can be a balancing act.\n\nCritical Value: 𝛼 = 0.05\n\nNow to run the t-test to see the results.\n\n# load packages: gapminder for data + tidyverse meta package\nlibrary(gapminder)\nlibrary(tidyverse)\n\n# filter data for life exp and countries: US and Netherlands\ndf1 <- gapminder %>%\n  select(country, lifeExp) %>%\n  filter(country == \"United States\" | country == \"Netherlands\") \n  \n t.test(data = df1, lifeExp ~ country)\n\n\n    Welch Two Sample t-test\n\ndata:  lifeExp by country\nt = 1.804, df = 20.316, p-value = 0.08608\nalternative hypothesis: true difference in means between group Netherlands and group United States is not equal to 0\n95 percent confidence interval:\n -0.3366519  4.6766519\nsample estimates:\n  mean in group Netherlands mean in group United States \n                    75.6485                     73.4785 \n\n\nThe results of our t-test show us that the Netherlands has a higher average life expectancy than the US, however with a p-value of .086 I will retain my null hypothesis as there is not a statistical significance in the difference of the means for each of the countries.\nWhat if I look at the life expectancy between the US and South Africa with the same parameters as before?\n\n# filter data for life exp and countries: US and South Africa\ndf2 <- gapminder %>%\n  select(country, lifeExp) %>%\n  filter(country == \"United States\" | country == \"South Africa\") \n  \n t.test(data = df2, lifeExp ~ country)\n\n\n    Welch Two Sample t-test\n\ndata:  lifeExp by country\nt = -10.549, df = 18.243, p-value = 3.392e-09\nalternative hypothesis: true difference in means between group South Africa and group United States is not equal to 0\n95 percent confidence interval:\n -23.36235 -15.60832\nsample estimates:\n mean in group South Africa mean in group United States \n                   53.99317                    73.47850 \n\n\nAs you can see there is a much larger difference in the average life expectancy between the countries, and with a p-value of 3.392e-09. This p-value is in scientific notation as it is such a small decimal, in decimal form it would be 0.000000003392 which is much smaller than .05 meaning this difference in life expectancy is statistically significant. In this case I would reject my null hypothesis and accept the alternative hypothesis that the average life expectancy of the United States is not equal to the average life expectancy of South Africa.\nIf I where reporting this insight, I could say that the life expectancy in the United States is significantly higher than that of South Africa… and instead of giving the p-value in decimal format or scientific notation, I could say p < .05."
  },
  {
    "objectID": "visualizing-data.html",
    "href": "visualizing-data.html",
    "title": "4  Visualizing Data",
    "section": "",
    "text": "There are many ways you can visualize data, and selecting a way to visualize your data depends on what kind of data you have. For example, if you have geographic data, then using a map can be an option. The visual you pick also should be effective at telling the story for your stakeholders.\nFor the different visualizations, I will group them based on what they show:"
  },
  {
    "objectID": "visualizing-data.html#distributions",
    "href": "visualizing-data.html#distributions",
    "title": "4  Visualizing Data",
    "section": "4.1 Distributions",
    "text": "4.1 Distributions\nFor this group the visuals all show the audience information about a distribution.\n\n4.1.1 Density Plot\nHere is a good resource that goes more in depth on density plots.\n\n# load in tidyverse package\nlibrary(tidyverse)\n\n# create some random data\nset.seed(0)\n\nn = 10000\n\nsample_means = rep(NA, n)\n\nfor(i in 1:n){\n  sample_means[i] = mean(rnorm(20, mean=0, sd=2))\n}\n\n# save this data into a data frame \nsample_means_df <- data.frame(sample_means)\n\n# create density plot \nsample_means_df %>%\n   ggplot(aes(sample_means)) + \n   geom_density(size = .5) +\n   labs(title = \"Density Plot\", x = \"Sample Means\", y = \"Density\") + \n   theme_classic()\n\n\n\n\n\n\n4.1.2 Density Ridges\nUsing the ggridges package you can compare and see distributions together. Click here for the package documentation.\n\n# create density ridges with 3 randomly sampled distributions \nsamples_df %>%\n  ggplot(aes(x = sample_means, y = sample_n, fill = sample_n)) +\n  geom_density_ridges(alpha = .7) + \n  labs(title = \"Density Ridges Plot\", x =\"Sample Means\", y = \"Sample ID\") +\n  theme_classic()\n\nPicking joint bandwidth of 0.0467\n\n\n\n\n\n\n\n4.1.3 Histogram\nUsing a histogram is another common way to show a distribution. It may look like a “bar char” with many bars, however each “bar” is a bin, and it represents a range of numbers that falls within it’s respective bin. The height of the “bar” shows a count of how many values fall within a bin.\n\n# create histogram with random data \nsample_means_df %>%\n  ggplot(aes(x = sample_means)) +\n  geom_histogram(bins = 30) +\n  labs(title = \"Histogram\", y = \"Count\", x = \"Sample Means\") +\n  theme_classic()\n\n\n\n\n\n\n4.1.4 Boxplot\nBoxplots can be a useful way to show a distribution, but the distribution is hidden behind each box meaning it could be misinterpreted.\n\n# create boxplots with random data\nsamples_df %>%\n  ggplot(aes(x = sample_n, y = sample_means, fill = sample_n)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot\") +\n  theme_classic()\n\n\n\n\n\n\n4.1.5 Violin Chart\nSimilar to a boxplot but shows the shape of a distribution better.\n\n# create violin charts with random data\nsamples_df %>%\n  ggplot(aes(x = sample_n, y = sample_means, fill = sample_n)) +\n  geom_violin() +\n  labs(title = \"Violin Chart\") +\n  theme_classic()"
  },
  {
    "objectID": "visualizing-data.html#comparisons",
    "href": "visualizing-data.html#comparisons",
    "title": "4  Visualizing Data",
    "section": "4.2 Comparisons",
    "text": "4.2 Comparisons\nFor this group the visuals compare insights for the audience.\n\n4.2.1 Bar Chart\nBar charts are very simple and effective at conveying information, never underestimate the power of a bar chart.\n\n# read in iris dataset \niris_df <- iris %>%\n  clean_names()\n\n# group the data by species, then summarize the avg petal width for each species\niris_df %>% \n  group_by(species) %>%\n  summarize(avg_petal_width = mean(petal_width)) %>%\n  ggplot(aes(x = species, y = avg_petal_width, fill = species)) +\n  geom_col() +\n  labs(title = \"Bar Chart\", y = \"Avg Petal Width\", x = \"Species\") +\n  theme_classic()\n\n\n\n\n\n\n4.2.2 Horizontal Bar Chart\nSimilar to a bar chart, but a horizontal version, can be useful but when viewing, stakeholders can more easily distinguish a difference in the vertical counterpart than in the horizontal bar chart. This is the same bar chart as above, created with + geom_col() but to rotate the plot I used the + coord_flip() function.\n\n# create horizontal bar chart\niris_df %>% \n  group_by(species) %>%\n  summarize(avg_petal_width = mean(petal_width)) %>%\n  ggplot(aes(x = species, y = avg_petal_width, fill = species)) +\n  geom_col() +\n  labs(title = \"Horizontal Bar Chart\", y = \"Avg Petal Width\", x = \"Species\") +\n  theme_classic() +\n  coord_flip()\n\n\n\n\n\n\n4.2.3 Line Chart\nLine charts are essential when working time.\n\n# read in chickweight dataset\nchick_df <- ChickWeight %>%\n  clean_names()\n\n# filter and group by chick 1, 21, 45\nchick_df %>%\n  filter(chick == 1|chick == 21|chick == 45) %>%\n  group_by(chick) %>%\n  ggplot(aes(x = time, y = weight, color = diet)) + \n  geom_line(size = .8) +\n  labs(title = \"Line Chart\", x = \"Time\", y = \"Chicken Weight\") +\n  theme_classic()"
  },
  {
    "objectID": "visualizing-data.html#relationships",
    "href": "visualizing-data.html#relationships",
    "title": "4  Visualizing Data",
    "section": "4.3 Relationships",
    "text": "4.3 Relationships\n…"
  },
  {
    "objectID": "visualizing-data.html#composition",
    "href": "visualizing-data.html#composition",
    "title": "4  Visualizing Data",
    "section": "4.4 Composition",
    "text": "4.4 Composition\n\n4.4.1 Pie Chart\nPie charts are useful when used properly, as a means of showing the composition categories etc. There should not be more than 5 items being displayed as it is hard to actualize the composition of something when there is lots of small pieces. 2-4 total “slices” is ideal for seeing how the data is made up for specific insights.\n\nlibrary(formattable)\n\niris_df %>%\n  group_by(species) %>%\n  summarise(cnt = n()) %>%\n  mutate(freq = formattable::percent(cnt / sum(cnt))) %>% \n  ggplot(aes(x = \"\", y = freq, fill = species)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  coord_polar(\"y\", start = 0) +\n  theme_void() +\n  geom_text(aes(label = paste0(freq)), position = position_stack(vjust=0.5)) +\n  labs(title = \"Pie Chart\")"
  },
  {
    "objectID": "collecting-data.html#twitter-scraping",
    "href": "collecting-data.html#twitter-scraping",
    "title": "1  Collecting Data",
    "section": "1.3 Twitter ‘Scraping’",
    "text": "1.3 Twitter ‘Scraping’\nTons of data is created on social media, and using social media can be a great source for businesses to see how their customers may feel about the goods or services they offer, advertise more products or intervene when a customer had a bad experience.\nYou can access data from Twitter by making a Twitter Developer’s Account and by using the Twitter API. After making a developers account it is very easy to get the data using the rtweet package. You can access the documentation for rtweet here.\nFirst you need to make sure you default browser is open and you are logged into your Twitter account. After you do this make sure to install the rtweet package and load it in RStudio.\n\ninstall.packages(\"rtweet\")\n\nlibrary(rtweet)\n\nAfter installing the package and loading it, you can use this code to set up your authentication and the search_tweets() function can retrieve Tweet data based on your parameters.\nIn this call of search_tweets() it will search for Tweets that contain “rocket league”, n = 100 means it will return 100 tweets, include_rts = FALSE means that it will not include and return Retweets, and finally lang = \"en\" means it will only return Tweets in English.\n\nauth_setup_default()\n\nauth_has_default()\n\nrt <- search_tweets(\"rocket league\", n = 100, include_rts = FALSE, lang = \"en\")"
  }
]