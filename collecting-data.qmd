# **Collecting Data**

The first step to any data analytics project is collecting data. This step may not be always necessary as in many cases the data has already been collected. For example in my current role we get daily analytics emails each morning that contain the data from the previous day. So if this data is relevant and all we need for our analysis we can skip the "collecting data" stage.

But what if we wanted to gain different insights that require new data? Then we would need to collect new data. Depending on what I was doing I could add more data to the current data I have, or create a new set of data all together.

## **Connecting to postgreSQL Database**

Connecting to a database can be an effective way to import data into R for analysis. For this example I will be showing how to connect to a postgreSQL database, as that is what I use.

You will need the `DBI`, `RPostgres`, and `dplyr` packages.

```{r eval=FALSE}
install.packages("DBI")
install.packages("RPostgres")
install.packages("dplyr")
```

Once you install the packages you will need to load them.

```{r eval=FALSE}
library(DBI)
library(RPostgres)
library(dplyr)
```

After loading the packages you can then connect to your database with this code and your database's credentials

```{r eval=FALSE}
# establish connection with postgres data base
con <- dbConnect(RPostgres::Postgres(),dbname = '',  # enter name of database in the single quotes
                 host = 'address', # replace 'address' and enter the address of the database inside the single quotes
                 port = 5432, # enter the port for the database 
                 user = 'username', # replace 'username' with your username inside single quotes
                 password = 'password') # replace 'password' with your password inside single quotes

# code to make sql query ---- inside of the double quotes you can create your query
res <- dbSendQuery(con, "
                   SELECT *
                   FROM 
                   WHERE ;
                   ")
# execute query
df <- dbFetch(res) # this will save the results of the query as an R data frame called 'DF'

# clear query
dbClearResult(res)

# disconnect from data base
dbDisconnect(con)
```

## **Web Scraping**

The way we collect data varies from using a pencil and notepad, to an automated process that saves every entry in the cloud. One way I have used R to collect data is by web scraping. Using the `rvest` package can allow you to collect data from the internet with minimal effort (avoid the constant copy/pasting).

Below I will show a simple script using the `rvest`, `lubridate` and `tidyverse` packages that can scrape us some data from [Steam's](https://store.steampowered.com/stats/) game stats page. Steam is a video game distribution service, and we will scrape a couple columns from their live *Top games by current player count* table.

```{r warning=FALSE,message=FALSE,error=FALSE}
# load packages
library(tidyverse)
library(rvest)
library(lubridate)

# link to get data from
link = "https://store.steampowered.com/stats/" 

# read webpage at the above link
page = read_html(link) 

# scrape top 100 games by current players
game = page %>% html_nodes(".gameLink") %>% html_text()  

# scrape number of players for each game 
current_players = page %>% html_nodes("td:nth-child(1) .currentServers") %>% html_text() 

# put both game and player data into a data frame
df = data.frame(game, current_players) 

# get current date
current_date <- as_datetime(Sys.Date())

# update data frame with mutated column that adds current_date
df <- df %>% 
  mutate(date = current_date)
 
```

Now lets see the first 6 rows of our new data frame.

```{r warning=FALSE,message=FALSE,error=FALSE}
head(df)
```
